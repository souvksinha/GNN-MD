{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECFP__dbDUtz"
   },
   "outputs": [],
   "source": [
    "## Run in py39_2 conda env\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "import math\n",
    "import sklearn\n",
    "import torch_optimizer as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR, ReduceLROnPlateau\n",
    "from metrics import *\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_max_pool as gmp, global_add_pool as gap,global_mean_pool as gep,global_sort_pool\n",
    "from torch_geometric.utils import dropout_adj\n",
    "import networkx as nx\n",
    "\n",
    "import biographs as bg\n",
    "from Bio import SeqIO\n",
    "from Bio.PDB.PDBParser import PDBParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87z8OLa0FGSj",
    "outputId": "0407afd2-9911-487a-b1a1-a2b3b3f6e19d"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Check if CUDA (GPU) is available and set the device accordingly\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wmn2Nxc5JNkZ",
    "outputId": "f63bad0c-5ee7-4cca-8ea4-9c530f32be0d"
   },
   "outputs": [],
   "source": [
    "# ftrs = np.load(\"cas13a_embedding.npy\", allow_pickle=True)\n",
    "# print(ftrs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2ZlDteXKTxh",
    "outputId": "19632796-a694-4084-a93d-f4610876e540"
   },
   "outputs": [],
   "source": [
    "import MDAnalysis as mda\n",
    "\n",
    "workdir = '/Users/souviksinha/Desktop/Palermo_Lab/LabWork/GNN_classifier/PPI_GNN/Chinmai_proj/win0_vs_win16/'\n",
    "\n",
    "# Load the DCD trajectory file and associated topology (PSF or PDB file)\n",
    "# dcd_file = os.path.join(workdir, \"below-US_window1.dcd\")\n",
    "# topology_file = os.path.join(workdir, \"SYS_nowat-ref.pdb\")  # or \"your_topology_file.pdb\"\n",
    "\n",
    "dcd_file = os.path.join(workdir, \"below-US_window16.dcd\")\n",
    "topology_file = os.path.join(workdir, \"SYS_nowat-ref.pdb\")  # or \"your_topology_file.pdb\"\n",
    "\n",
    "# os.makedirs(os.path.join(workdir, \"frame_1\"), exist_ok = True)\n",
    "os.makedirs(os.path.join(workdir, \"frame_16\"), exist_ok = True)\n",
    "\n",
    "# Create a Universe object to represent the system\n",
    "u = mda.Universe(topology_file, dcd_file)\n",
    "\n",
    "# Iterate over each frame in the trajectory\n",
    "for ts in u.trajectory:\n",
    "    # Create a PDB filename for the current frame (you can modify the naming as needed)\n",
    "    pdb_filename = os.path.join(workdir,f\"frame_16/frame_{ts.frame}.pdb\")\n",
    "    ag = u.select_atoms(\"protein\")\n",
    "\n",
    "    # Write the coordinates of the current frame to the PDB file\n",
    "    with mda.Writer(pdb_filename, bonds=None, n_atoms=ag.atoms.n_atoms) as pdb:\n",
    "        pdb.write(ag.atoms)\n",
    "\n",
    "    print(f\"Saved PDB file: {pdb_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "e7IiEEQ-Q6Ca",
    "outputId": "0498a93a-d42c-4bf5-863d-e4a2a7de624d"
   },
   "outputs": [],
   "source": [
    "from bio_embeddings.embed import SeqVecEmbedder\n",
    "\n",
    "workdir = '/Users/souviksinha/Desktop/Palermo_Lab/LabWork/GNN_classifier/PPI_GNN/Chinmai_proj/win0_vs_win16_win56/'\n",
    "# Dictionary for getting Residue symbols\n",
    "ressymbl = {'ALA': 'A', 'CYS': 'C', 'ASP': 'D', 'GLU':'E', 'PHE': 'F', 'GLY': 'G', 'HIE': 'H', 'ILE': 'I', 'LYS': 'K', 'LEU': 'L', 'MET': 'M', 'ASN': 'N', 'PRO': 'P', 'GLN':'Q', 'ARG':'R', 'SER': 'S','THR': 'T', 'VAL': 'V', 'TRP':'W', 'TYR': 'Y'}\n",
    "# get structure from a pdb file\n",
    "# Uses biopython\n",
    "def get_structure(file):\n",
    "    parser = PDBParser()\n",
    "    structure = parser.get_structure(id, file) # return a Structure object.\n",
    "    return structure\n",
    "\n",
    "# Function to get sequence from pdb structure\n",
    "# Uses structure made using biopython\n",
    "# Those residues for which symbols are U / X are converted into A\n",
    "def get_sequence(structure):\n",
    "    sequence =\"\"\n",
    "    for model in structure:\n",
    "      for chain in model:\n",
    "        for residue in chain:\n",
    "          if residue.get_resname() in ressymbl.keys():\n",
    "              sequence = sequence+ ressymbl[residue.get_resname()]\n",
    "    return sequence\n",
    "\n",
    "# Define the protein sequence\n",
    "# seq = 'MKVTKVGGISHKKYTSEGRLVKSESEENRTDERLSALLNMRLDMYIKNPSSTETKENQKRIGKLKKFFSNKMVYLKDNTLSLKNGKKENIDREYSETDILESDVRDKKNFAVLKKIYLNENVNSEELEVFRNDIKKKLNKINSLKYSFEKNKANYQKINENNIEKVEGKSKRNIIYDYYRESAKRDAYVSNVKEAFDKLYKEEDIAKLVLEIENLTKLEKYKIREFYHEIIGRKNDKENFAKIIYEEIQNVNNMKELIEKVPDMSELKKSQVFYKYYLDKEELNDKNIKYAFCHFVEIEMSQLLKNYVYKRLSNISNDKIKRIFEYQNLKKLIENKLLNKLDTYVRNCGKYNYYLQDGEIATSDFIARNRQNEAFLRNIIGVSSVAYFSLRNILETENENDITGRMRGKTVKNNKGEEKYVSGEVDKIYNENKKNEVKENLKMFYSYDFNMDNKNEIEDFFANIDEAISSIRHGIVHFNLELEGKDIFAFKNIAPSEISKKMFQNEINEKKLKLKIFRQLNSANVFRYLEKYKILNYLKRTRFEFVNKNIPFVPSFTKLYSRIDDLKNSLGIYWKTPKTNDDNKTKEIIDAQIYLLKNIYYGEFLNYFMSNNGNFFEISKEIIELNKNDKRNLKTGFYKLQKFEDIQEKIPKEYLANIQSLYMINAGNQDEEEKDTYIDFIQKIFLKGFMTYLANNGRLSLIYIGSDEETNTSLAEKKQEFDKFLKKYEQNNNIKIPYEINEFLREIKLGNILKYTERLNMFYLILKLLNHKELTNLKGSLEKYQSANKEEAFSDQLELINLLNLDNNRVTEDFELEADEIGKFLDFNGNKVKDNKELKKFDTNKIYFDGENIIKHRAFYNIKKYGMLNLLEKIADKAGYKISIEELKKYSNKKNEIEKNHKMQENLHRKYARPRKDEKFTDEDYESYKQAIENIEEYTHLKNKVEFNELNLLQGLLLRILHRLVGYTSIWERDLRFRLKGEFPENQYIEEIFNFENKKNVKYKGGQIVEKYIKFYKELHQNDEVKINKYSSANIKVLKQEKKDLYIANYIAAFNYIPHAEISLLEVLENLRKLLSYDRKLKNAVMKSVVDILKEYGFVATFKIGADKKIGIQTLESEKIVHLKNLKKKKLMTDRNSEELCKLVKIMFEYKMEEKKSEN'\n",
    "seq = get_sequence(get_structure(os.path.join(workdir, \"SYS_nowat-ref.pdb\")))\n",
    "# Create an instance of the SeqVecEmbedder\n",
    "embedder = SeqVecEmbedder()\n",
    "\n",
    "# Obtain the embedding of the protein sequence\n",
    "embedding = embedder.embed(seq)\n",
    "\n",
    "# Convert the embedding to a PyTorch tensor on CPU\n",
    "protein_embd = torch.tensor(embedding).sum(dim=0).cpu().numpy()\n",
    "print(protein_embd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdbgbf3wYzZx"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Dataset, Dataset, download_url, Data,  Batch\n",
    "\n",
    "# list of 20 proteins\n",
    "pro_res_table = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "\n",
    "# Dictionary for getting Residue symbols\n",
    "ressymbl = {'ALA': 'A', 'CYS': 'C', 'ASP': 'D', 'GLU':'E', 'PHE': 'F', 'GLY': 'G', 'HIE': 'H', 'ILE': 'I', 'LYS': 'K', 'LEU': 'L', 'MET': 'M', 'ASN': 'N', 'PRO': 'P', 'GLN':'Q', 'ARG':'R', 'SER': 'S','THR': 'T', 'VAL': 'V', 'TRP':'W', 'TYR': 'Y'}\n",
    "\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(ProteinDataset, self).__init__(root, transform=None,\n",
    "                 pre_transform=None)\n",
    "        self.data = self.processed_paths\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "\n",
    "#         return [filename for filename in os.scandir(self.root)] # A list of files in the raw directory which needs to be found in order to skip the download. (this file path is also raw_path)\n",
    "        files = [filename for filename in os.scandir(self.root+\"/raw\")] # so keep all the pdb files in the raw/ directory\n",
    "#         print(files)\n",
    "#         print(self.raw_paths)\n",
    "        return files\n",
    "\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "#         print(self.raw_paths)\n",
    "        file_names = [file.split('/')[0]+\"/\"+file.split('/')[1]+\"/\"+file.split('/')[4] for file in self.raw_paths]\n",
    "#         file = self.raw_paths[0]\n",
    "#         print(file_names)\n",
    "#         return [os.path.splitext(os.path.basename(file))[0]+'.pt' for file in self.raw_paths] # A list of files in the processed_dir which needs to be found in order to skip the processing. If *.pt not found, data will be processed\n",
    "        return [os.path.splitext(os.path.basename(file))[0]+'.pt' for file in file_names]\n",
    "    def download(self):\n",
    "        # Download to `self.raw_dir`.\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        self.data = self.processed_paths\n",
    "\n",
    "\n",
    "        data_list =[]\n",
    "        count = 0\n",
    "        file_names = [file.split('/')[0]+\"/\"+file.split('/')[1]+\"/\"+file.split('/')[4] for file in self.raw_paths]\n",
    "#         for file in tqdm(self.raw_paths): # tqdm is a progress bar library that can be used to visualize progress when iterating through a sequence (in this case, file paths).\n",
    "        for file in tqdm(file_names): # tqdm is a progress bar library that can be used to visualize progress when iterating through a sequence (in this case, file paths).\n",
    "           if(pathlib.Path(file).suffix ==\".pdb\"): # checks if the current file has a \".pdb\" extension. This is done using pathlib.Path(file).suffix, which returns the file extension.\n",
    "\n",
    "               try:\n",
    "                struct = self._get_structure(file) # extract structural information from the PDB file (file)\n",
    "               except:\n",
    "                print('except', file)\n",
    "                continue\n",
    "               seq = self._get_sequence(struct) # extract the sequence information from the structural information (struct)\n",
    "\n",
    "          # node features extracted\n",
    "              #  node_feats = self._get_one_hot_symbftrs(seq) # extract node features (one-hot encoded symbol features) from the sequence (seq)\n",
    "               node_feats = torch.tensor(protein_embd) # extract node features (one-hot encoded symbol features) from the sequence (seq)\n",
    "\n",
    "          #edge-index extracted\n",
    "\n",
    "\n",
    "               mat = self._get_adjacency(file) # extract the adjacency matrix (mat) from the PDB file (file\n",
    "\n",
    "           # if sequence size > matrix dimensions\n",
    "               if(mat.shape[0] < torch.Tensor.size(node_feats)[0]) :\n",
    "                 #node_feats = torch.tensor(ftrs.item()[os.path.splitext(os.path.basename(file))[0]])\n",
    "                 edge_index = self._get_edgeindex(file, mat)\n",
    "\n",
    "                 print(f'Node features size :{torch.Tensor.size(node_feats)}')\n",
    "                 print(f'mat size :{mat.shape}')\n",
    "          # create data object\n",
    "\n",
    "                 data = Data(x = node_feats, edge_index = edge_index )\n",
    "                 count += 1\n",
    "                 data_list.append(data)\n",
    "                 torch.save(data, self.processed_dir + \"/\"+ os.path.splitext(os.path.basename(file))[0]+'.pt')\n",
    "\n",
    "\n",
    "               elif mat.shape[0] == torch.Tensor.size(node_feats)[0] :\n",
    "                 #node_feats = torch.tensor(ftrs.item()[os.path.splitext(os.path.basename(file))[0]])\n",
    "                 edge_index = self._get_edgeindex(file, mat)\n",
    "\n",
    "\n",
    "                 print(f'Node features size :{torch.Tensor.size(node_feats)}')\n",
    "                 print(f'mat size :{mat.shape}')\n",
    "\n",
    "          # create data object\n",
    "\n",
    "                 data = Data(x = node_feats, edge_index = edge_index )\n",
    "                 count += 1\n",
    "\n",
    "                 data_list.append(data)\n",
    "                 torch.save(data, self.processed_dir + \"/\"+ os.path.splitext(os.path.basename(file))[0]+'.pt')\n",
    "\n",
    "        self.data_prot = data_list\n",
    "        print(count)\n",
    "\n",
    "\n",
    "        # data, slices = self.collate(data_list)\n",
    "        # torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "\n",
    "    # file stands for file path\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.data_prot[idx]\n",
    "\n",
    "    # Biographs returns the network as a networkx.Graph object\n",
    "    def _get_adjacency(self, file):\n",
    "        edge_ind =[]\n",
    "        molecule = bg.Pmolecule(file)\n",
    "        network = molecule.network() # network.nodes[:10] can be used for output residue nodes; network = molecule.network(cutoff=8) to increase distance cut-off from 5A (default to 8A)\n",
    "        mat = nx.adjacency_matrix(network) # calculates the adjacency matrix of a NetworkX graph ()\n",
    "        m = mat.todense() # convert to dense matrix (probably internally using scipy library)\n",
    "        return m\n",
    "\n",
    "\n",
    "    # get adjacency matrix in coo format to pass in GCNN model\n",
    "    def _get_edgeindex(self, file, adjacency_mat):\n",
    "        edge_ind = []\n",
    "        m = self._get_adjacency(file) #\n",
    "        #check_symmetric(m, rtol=1e-05, atol=1e-08)\n",
    "\n",
    "        a = np.nonzero(m > 0)[0] # find the indices of nonzero elements in the matrix m. [row]\n",
    "        b = np.nonzero(m > 0)[1] # find the indices of nonzero elements in the matrix m. [column]\n",
    "        edge_ind.append(a) # These arrays represent the edge indices of the nonzero elements in the adjacency matrix.\n",
    "        edge_ind.append(b)\n",
    "        return torch.tensor(np.array(edge_ind), dtype= torch.long) # creates a PyTorch tensor from the concatenated arrays in edge_ind, converting it to a long data type tensor\n",
    "\n",
    "\n",
    "    # get structure from a pdb file\n",
    "    # Uses biopython\n",
    "    def _get_structure(self, file):\n",
    "        parser = PDBParser()\n",
    "        structure = parser.get_structure(id, file) # return a Structure object.\n",
    "        return structure\n",
    "\n",
    "    # Function to get sequence from pdb structure\n",
    "    # Uses structure made using biopython\n",
    "    # Those residues for which symbols are U / X are converted into A\n",
    "    def _get_sequence(self, structure):\n",
    "        sequence =\"\"\n",
    "        for model in structure:\n",
    "          for chain in model:\n",
    "            for residue in chain:\n",
    "              if residue.get_resname() in ressymbl.keys():\n",
    "                  sequence = sequence+ ressymbl[residue.get_resname()]\n",
    "        return sequence\n",
    "\n",
    "\n",
    "    # One hot encoding for symbols\n",
    "    def _get_one_hot_symbftrs(self, sequence):\n",
    "        one_hot_symb = np.zeros((len(sequence),len(pro_res_table)))\n",
    "        row= 0\n",
    "        for res in sequence:\n",
    "          col = pro_res_table.index(res)\n",
    "          one_hot_symb[row][col]=1\n",
    "          row +=1\n",
    "        return torch.tensor(one_hot_symb, dtype= torch.float)\n",
    "\n",
    "\n",
    "    # Residue features calculated from pcp_dict\n",
    "    def _get_res_ftrs(self, sequence):\n",
    "        res_ftrs_out = []\n",
    "        for res in sequence:\n",
    "          res_ftrs_out.append(pcp_dict[res])\n",
    "        res_ftrs_out= np.array(res_ftrs_out)\n",
    "        #print(res_ftrs_out.shape)\n",
    "        return torch.tensor(res_ftrs_out, dtype = torch.float)\n",
    "\n",
    "\n",
    "    # total features after concatenating one_hot_symbftrs and res_ftrs\n",
    "    def _get_node_ftrs(self, sequence):\n",
    "        one_hot_symb = one_hot_symbftrs(sequence)\n",
    "        res_ftrs_out = res_ftrs(sequence)\n",
    "        return torch.tensor(np.hstack((one_hot_symb, res_ftrs_out)), dtype = torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# molecule = bg.Pmolecule(\"/Users/souviksinha/Desktop/Palermo_Lab/LabWork/GNN_classifier/PPI_GNN/Chinmai_proj/frame_1/frame_100.pdb\")\n",
    "# network = molecule.network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network.nodes[:10]\n",
    "# print(network)\n",
    "# network.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HabVZ86yLert"
   },
   "outputs": [],
   "source": [
    "# prot_graphs = ProteinDataset(root=\"frame_1/\")\n",
    "# prot_graphs = ProteinDataset(root=\"frame_16/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trainset.indices)\n",
    "# trainloader = DataLoader(dataset=trainset, batch_size=64, num_workers=0)\n",
    "# print(trainloader.batch_size)\n",
    "# for batch in trainloader:\n",
    "# #     print('a')\n",
    "#     data, labels = batch\n",
    "#     print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "\n",
    "class ManuallyLabelledDataset(Dataset):\n",
    "    def __init__(self, label0_dir, label1_dir, label2_dir):\n",
    "        self.label0_files = glob.glob(os.path.join(label0_dir, \"frame*.pt\"))\n",
    "        self.label1_files = glob.glob(os.path.join(label1_dir, \"frame*.pt\"))\n",
    "        self.label2_files = glob.glob(os.path.join(label2_dir, \"frame*.pt\"))\n",
    "\n",
    "        self.files = self.label0_files + self.label1_files + self.label2_files\n",
    "        self.labels = [0] * len(self.label0_files) + [1] * len(self.label1_files) + [2] * len(self.label2_files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        data = torch.load(file_path)\n",
    "        # data.y = torch.tensor([label], dtype=torch.float32)\n",
    "\n",
    "        # Convert label to a tensor (dtype long for multi-class classification)\n",
    "        data.y = torch.tensor([label], dtype=torch.long)\n",
    "        \n",
    "        return data\n",
    "\n",
    "# Define paths to the directories\n",
    "label0_dir = \"frame_1/processed\"\n",
    "label1_dir = \"frame_16/processed\"\n",
    "label2_dir = \"frame_56/processed\"\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = ManuallyLabelledDataset(label0_dir=label0_dir, label1_dir=label1_dir, label2_dir=label2_dir)\n",
    "\n",
    "# Split into training and testing sets\n",
    "print(\"Size is:\", len(dataset))\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "# train_size = 256\n",
    "test_size = len(dataset) - train_size\n",
    "print(train_size, test_size)\n",
    "\n",
    "trainset, testset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=16, shuffle=True) # always keep in mind that each batch should met batch size criterio i.e. number of batches * batch_size = trainloader\n",
    "testloader = DataLoader(dataset=testset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Verify the batched data\n",
    "for step, batch in enumerate(trainloader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {batch.num_graphs}')\n",
    "    print(f'Node feature shape: {batch.x.shape}')\n",
    "    print(f'Edge index shape: {batch.edge_index.shape}')\n",
    "    print(f'Batch tensor: {batch.batch}') \n",
    "    print(f'Label tensor: {batch.y}')\n",
    "    print(batch)\n",
    "    print()\n",
    "    break  # Remove this if you want to see all batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Node feature shape: torch.Size([11530, 1024])\n",
    "The node features for all nodes in the batch (10*1153), with each node having 1024 features. The first dimension (11530) represents the total number of nodes across all graphs in the batch.\n",
    "2. Batch tensor: tensor([0, 0, 0,  ..., 9, 9, 9])\n",
    "The batch tensor is a 1-dimensional tensor where each element indicates the graph index for the corresponding node.\n",
    "3. Edge index shape: torch.Size([2, 145596])\n",
    "The edge indices for all edges in the batch. The first dimension (2) is because edges are represented as pairs of node indices (source and target). The second dimension (145734) is the total number of edges in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features_pro, hidden_channels, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "#         torch.manual_seed(12345)\n",
    "#         self.conv1 = GCNConv(num_features_pro, hidden_channels)\n",
    "        self.conv1 = GCNConv(num_features_pro, num_features_pro)\n",
    "#         self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(num_features_pro, hidden_channels)\n",
    "        self.fc1 = Linear(hidden_channels, 128)\n",
    "        \n",
    "        self.fc2 = Linear(128, 64)\n",
    "        self.out = Linear(64, num_classes)\n",
    "        self.relu = torch.nn.LeakyReLU()\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "#         x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # add some dense layers\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.out(x)\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN(num_features_pro=1024, hidden_channels=256, num_classes=3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes, heads=4):\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "\n",
    "        # First Graph Attention Layer with multi-head attention\n",
    "        self.gat1 = GATConv(num_features_pro, hidden_channels, heads=heads, concat=True)\n",
    "        # Second Graph Attention Layer\n",
    "        self.gat2 = GATConv(hidden_channels * heads, hidden_channels, heads=1, concat=True)  # 1 head, no concatenation\n",
    "        \n",
    "        # Fully connected layer to map hidden states to output classes\n",
    "        self.fc1 = Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # First GAT layer with ReLU activation\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Second GAT layer with ReLU activation\n",
    "        x = self.gat2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Global Mean Pooling to get graph-level representation\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "        # Return log probabilities with softmax for 3-class classification\n",
    "        # return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "# Initialize model with input feature size (in_channels), hidden channels, and 3 output classes\n",
    "num_features_pro = 1024  # Example input size\n",
    "hidden_channels = 256\n",
    "num_classes = 3\n",
    "heads = 4  # Multi-head attention\n",
    "\n",
    "model = GAT(num_features_pro, hidden_channels, num_classes, heads)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we load only then run this\n",
    "model.load_state_dict(torch.load('best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer =  torch.optim.Adam(model.parameters(), lr= 0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    scheduler = MultiStepLR(optimizer, milestones=[5,8], gamma=0.5)\n",
    "\n",
    "    for data in trainloader:  # Iterate in batches over the training dataset.\n",
    "        out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "#         print(out)\n",
    "        label = data.y.long().to(device)\n",
    "        loss = criterion(out, label)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "    scheduler.step()\n",
    "    print(f'Epoch {epoch}  [==============================] - train_loss : {loss}')\n",
    "        \n",
    "            \n",
    "def test(loader):\n",
    "        model.eval()\n",
    "\n",
    "        correct = 0\n",
    "        for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "            out = model(data.x, data.edge_index, data.batch)  \n",
    "            pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    train_acc = test(trainloader)\n",
    "    test_acc = test(testloader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is to check that if all batches have same number of data\n",
    "# c = 0\n",
    "# for data in trainloader:  # Iterate in batches over the training dataset.\n",
    "#         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "# #         print(out)\n",
    "#         label = data.y.long().to(device)\n",
    "#         print(label)\n",
    "#         print(c)\n",
    "#         c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Early stopping parameters\n",
    "# patience = 20 # GCNN\n",
    "patience = 5 # GAT\n",
    "best_test_acc = 0\n",
    "epochs_no_improve = 0\n",
    "early_stop = False\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    \n",
    "    # scheduler = MultiStepLR(optimizer, milestones=[18, 22], gamma=0.5) # GCNN\n",
    "    scheduler = MultiStepLR(optimizer, milestones=[6, 9], gamma=0.5) #GAT\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in trainloader:  # Iterate in batches over the training dataset.\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "        \n",
    "        # Assuming the model output is of shape [batch_size, 2]\n",
    "        label = data.y.long().to(device)  # Ensure labels are of type long\n",
    "        loss = criterion(out, label)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        total_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / len(trainloader)\n",
    "    print(f'Epoch {epoch}  [==============================] - train_loss : {avg_loss:.4f}')\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in loader:  # Iterate in batches over the dataset.\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.batch)  \n",
    "            \n",
    "            # Use softmax to get probabilities\n",
    "            # prob = torch.softmax(out, dim=1)\n",
    "            prob = torch.log_softmax(out, dim=-1)\n",
    "            pred = prob.argmax(dim=1)  # Use the class with highest probability\n",
    "            \n",
    "            y_true.extend(data.y.cpu().numpy())\n",
    "            y_pred.extend(pred.cpu().numpy())\n",
    "\n",
    "            \n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # precision = precision_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='micro')\n",
    "    # recall = recall_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred,average='micro')\n",
    "    f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    # roc_auc = roc_auc_score(y_true, y_pred, multi_class='ovr')\n",
    "    \n",
    "    # return accuracy, precision, recall, f1, roc_auc\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# for epoch in range(1, 25): # GCNN\n",
    "for epoch in range(1, 10): # GAT\n",
    "    train(epoch)\n",
    "    # train_acc, train_prec, train_recall, train_f1, train_roc_auc = evaluate(trainloader)\n",
    "    # test_acc, test_prec, test_recall, test_f1, test_roc_auc = evaluate(testloader)\n",
    "\n",
    "    train_acc, train_prec, train_recall, train_f1 = evaluate(trainloader)\n",
    "    test_acc, test_prec, test_recall, test_f1 = evaluate(testloader)\n",
    "    \n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    print(f'Precision: Train: {train_prec:.4f}, Test: {test_prec:.4f}')\n",
    "    print(f'Recall: Train: {train_recall:.4f}, Test: {test_recall:.4f}')\n",
    "    print(f'F1-Score: Train: {train_f1:.4f}, Test: {test_f1:.4f}')\n",
    "    # print(f'ROC-AUC: Train: {train_roc_auc:.4f}, Test: {test_roc_auc:.4f}')\n",
    "    \n",
    "    # Early stopping logic\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        epochs_no_improve = 0\n",
    "        # torch.save(model.state_dict(), 'best_model.pt') # GCNN\n",
    "        torch.save(model.state_dict(), 'best_model_GAT.pt') # GAT\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print('Early stopping!')\n",
    "            early_stop = True\n",
    "            break\n",
    "\n",
    "# Load the best model if early stopping was triggered\n",
    "if early_stop:\n",
    "    # model.load_state_dict(torch.load('best_model.pt')) # GCNN\n",
    "    model.load_state_dict(torch.load('best_model_GAT.pt')) # GAT\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
