{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECFP__dbDUtz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "import math\n",
    "import sklearn\n",
    "import torch_optimizer as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR, ReduceLROnPlateau\n",
    "from metrics import *\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_max_pool as gmp, global_add_pool as gap,global_mean_pool as gep,global_sort_pool\n",
    "from torch_geometric.utils import dropout_adj\n",
    "import networkx as nx\n",
    "\n",
    "import biographs as bg\n",
    "from Bio import SeqIO\n",
    "from Bio.PDB.PDBParser import PDBParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87z8OLa0FGSj",
    "outputId": "0407afd2-9911-487a-b1a1-a2b3b3f6e19d"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Check if CUDA (GPU) is available and set the device accordingly\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "e7IiEEQ-Q6Ca",
    "outputId": "0498a93a-d42c-4bf5-863d-e4a2a7de624d"
   },
   "outputs": [],
   "source": [
    "from bio_embeddings.embed import SeqVecEmbedder\n",
    "\n",
    "workdir = '/Users/souviksinha/Desktop/Palermo_Lab/LabWork/GNN_classifier/PPI_GNN/WT_R963A_test'\n",
    "# Dictionary for getting Residue symbols\n",
    "ressymbl = {'ALA': 'A', 'CYS': 'C', 'ASP': 'D', 'GLU':'E', 'PHE': 'F', 'GLY': 'G', 'HIE': 'H', 'ILE': 'I', 'LYS': 'K', 'LEU': 'L', 'MET': 'M', 'ASN': 'N', 'PRO': 'P', 'GLN':'Q', 'ARG':'R', 'SER': 'S','THR': 'T', 'VAL': 'V', 'TRP':'W', 'TYR': 'Y'}\n",
    "\n",
    "# get structure from a pdb file\n",
    "# Uses biopython\n",
    "def get_structure(file):\n",
    "    parser = PDBParser()\n",
    "    structure = parser.get_structure(id, file) # return a Structure object.\n",
    "    return structure\n",
    "\n",
    "# Function to get sequence from pdb structure\n",
    "# Uses structure made using biopython\n",
    "# Those residues for which symbols are U / X are converted into A\n",
    "def get_sequence(structure):\n",
    "    sequence =\"\"\n",
    "    for model in structure:\n",
    "      for chain in model:\n",
    "        for residue in chain:\n",
    "          if residue.get_resname() in ressymbl.keys():\n",
    "              sequence = sequence+ ressymbl[residue.get_resname()]\n",
    "    return sequence\n",
    "\n",
    "# Define the protein sequence\n",
    "# seq = 'MKVTKVGGISHKKYTSEGRLVKSESEENRTDERLSALLNMRLDMY' \n",
    "# seq = get_sequence(get_structure(os.path.join(workdir, \"R963A/raw/frame_1480.pdb\")))\n",
    "seq = get_sequence(get_structure(os.path.join(workdir, \"WT/raw/frame_1480.pdb\")))\n",
    "# Create an instance of the SeqVecEmbedder\n",
    "embedder = SeqVecEmbedder()\n",
    "\n",
    "# Obtain the embedding of the protein sequence\n",
    "embedding = embedder.embed(seq)\n",
    "\n",
    "# Convert the embedding to a PyTorch tensor on CPU\n",
    "protein_embd = torch.tensor(embedding).sum(dim=0).cpu().numpy()\n",
    "print(protein_embd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MDAnalysis as mda\n",
    "import glob\n",
    "\n",
    "# workdir = '/Users/souviksinha/Desktop/Palermo_Lab/LabWork/GNN_classifier/PPI_GNN/WT_R963A_test/R963A_cluster'\n",
    "workdir = '/Users/souviksinha/Desktop/Palermo_Lab/LabWork/GNN_classifier/PPI_GNN/WT_R963A_test/WT_cluster'\n",
    "\n",
    "pdb_files = glob.glob(os.path.join(workdir,'*.pdb'))\n",
    "print(pdb_files)\n",
    "print(pdb_files[0].split('/')[-1])\n",
    "\n",
    "for pdb in pdb_files:\n",
    "    u = mda.Universe(pdb)\n",
    "    name  = pdb.split('/')[-1]\n",
    "    pdb_filename = os.path.join(workdir, f\"raw/{name}\")\n",
    "\n",
    "    ag = u.select_atoms(\"protein\")\n",
    "\n",
    "    # Write the coordinates of the current frame to the PDB file\n",
    "    with mda.Writer(pdb_filename, bonds=None, n_atoms=ag.atoms.n_atoms) as pdb:\n",
    "        pdb.write(ag.atoms)\n",
    "\n",
    "    print(f\"Saved PDB file: {pdb_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Get the protein graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdbgbf3wYzZx"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Dataset, Dataset, download_url, Data,  Batch\n",
    "\n",
    "# list of 20 proteins\n",
    "pro_res_table = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "\n",
    "# Dictionary for getting Residue symbols\n",
    "ressymbl = {'ALA': 'A', 'CYS': 'C', 'ASP': 'D', 'GLU':'E', 'PHE': 'F', 'GLY': 'G', 'HIE': 'H', 'ILE': 'I', 'LYS': 'K', 'LEU': 'L', 'MET': 'M', 'ASN': 'N', 'PRO': 'P', 'GLN':'Q', 'ARG':'R', 'SER': 'S','THR': 'T', 'VAL': 'V', 'TRP':'W', 'TYR': 'Y'}\n",
    "\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(ProteinDataset, self).__init__(root, transform=None,\n",
    "                 pre_transform=None)\n",
    "        self.data = self.processed_paths\n",
    "        #self.processed_dir = \"../human_features/processed/\"\n",
    "        # self.data = torch.load(self.processed_paths)\n",
    "        # print(\"Daata si {}\". format(self.data))\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "\n",
    "#         return [filename for filename in os.scandir(self.root)] # A list of files in the raw directory which needs to be found in order to skip the download. (this file path is also raw_path)\n",
    "        files = [filename for filename in os.scandir(self.root+\"/raw\")]\n",
    "#         print(files)\n",
    "#         print(self.raw_paths)\n",
    "        return files\n",
    "\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "#         print(self.raw_paths)\n",
    "        file_names = [file.split('/')[0]+\"/\"+file.split('/')[1]+\"/\"+file.split('/')[4] for file in self.raw_paths]\n",
    "#         file = self.raw_paths[0]\n",
    "#         print(file_names)\n",
    "#         return [os.path.splitext(os.path.basename(file))[0]+'.pt' for file in self.raw_paths] # A list of files in the processed_dir which needs to be found in order to skip the processing. If *.pt not found, data will be processed\n",
    "        return [os.path.splitext(os.path.basename(file))[0]+'.pt' for file in file_names]\n",
    "    def download(self):\n",
    "        # Download to `self.raw_dir`.\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        self.data = self.processed_paths\n",
    "\n",
    "\n",
    "        data_list =[]\n",
    "        count = 0\n",
    "        file_names = [file.split('/')[0]+\"/\"+file.split('/')[1]+\"/\"+file.split('/')[4] for file in self.raw_paths]\n",
    "#         for file in tqdm(self.raw_paths): # tqdm is a progress bar library that can be used to visualize progress when iterating through a sequence (in this case, file paths).\n",
    "        for file in tqdm(file_names): # tqdm is a progress bar library that can be used to visualize progress when iterating through a sequence (in this case, file paths).\n",
    "           if(pathlib.Path(file).suffix ==\".pdb\"): # checks if the current file has a \".pdb\" extension. This is done using pathlib.Path(file).suffix, which returns the file extension.\n",
    "\n",
    "               try:\n",
    "                struct = self._get_structure(file) # extract structural information from the PDB file (file)\n",
    "               except:\n",
    "                print('except', file)\n",
    "                continue\n",
    "               seq = self._get_sequence(struct) # extract the sequence information from the structural information (struct)\n",
    "\n",
    "          # node features extracted\n",
    "              #  node_feats = self._get_one_hot_symbftrs(seq) # extract node features (one-hot encoded symbol features) from the sequence (seq)\n",
    "               node_feats = torch.tensor(protein_embd) # extract node features (one-hot encoded symbol features) from the sequence (seq)\n",
    "\n",
    "          #edge-index extracted\n",
    "\n",
    "\n",
    "               mat = self._get_adjacency(file) # extract the adjacency matrix (mat) from the PDB file (file\n",
    "\n",
    "           # if sequence size > matrix dimensions\n",
    "               if(mat.shape[0] < torch.Tensor.size(node_feats)[0]) :\n",
    "                 #node_feats = torch.tensor(ftrs.item()[os.path.splitext(os.path.basename(file))[0]])\n",
    "                 edge_index = self._get_edgeindex(file, mat)\n",
    "\n",
    "                 print(f'Node features size :{torch.Tensor.size(node_feats)}')\n",
    "                 print(f'mat size :{mat.shape}')\n",
    "          # create data object\n",
    "\n",
    "                 data = Data(x = node_feats, edge_index = edge_index )\n",
    "                 count += 1\n",
    "                 data_list.append(data)\n",
    "                 torch.save(data, self.processed_dir + \"/\"+ os.path.splitext(os.path.basename(file))[0]+'.pt')\n",
    "\n",
    "\n",
    "               elif mat.shape[0] == torch.Tensor.size(node_feats)[0] :\n",
    "                 #node_feats = torch.tensor(ftrs.item()[os.path.splitext(os.path.basename(file))[0]])\n",
    "                 edge_index = self._get_edgeindex(file, mat)\n",
    "\n",
    "\n",
    "                 print(f'Node features size :{torch.Tensor.size(node_feats)}')\n",
    "                 print(f'mat size :{mat.shape}')\n",
    "\n",
    "          # create data object\n",
    "\n",
    "                 data = Data(x = node_feats, edge_index = edge_index )\n",
    "                 count += 1\n",
    "\n",
    "                 data_list.append(data)\n",
    "                 torch.save(data, self.processed_dir + \"/\"+ os.path.splitext(os.path.basename(file))[0]+'.pt')\n",
    "\n",
    "        self.data_prot = data_list\n",
    "        print(count)\n",
    "\n",
    "\n",
    "        # data, slices = self.collate(data_list)\n",
    "        # torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "\n",
    "    # file stands for file path\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.data_prot[idx]\n",
    "\n",
    "    # Biographs returns the network as a networkx.Graph object\n",
    "    def _get_adjacency(self, file):\n",
    "        edge_ind =[]\n",
    "        molecule = bg.Pmolecule(file)\n",
    "        network = molecule.network() # network.nodes[:10] can be used for output residue nodes; network = molecule.network(cutoff=8) to increase distance cut-off from 5A (default to 8A)\n",
    "        mat = nx.adjacency_matrix(network) # calculates the adjacency matrix of a NetworkX graph ()\n",
    "        m = mat.todense() # convert to dense matrix (probably internally using scipy library)\n",
    "        return m\n",
    "\n",
    "\n",
    "    # get adjacency matrix in coo format to pass in GCNN model\n",
    "    def _get_edgeindex(self, file, adjacency_mat):\n",
    "        edge_ind = []\n",
    "        m = self._get_adjacency(file) #\n",
    "        #check_symmetric(m, rtol=1e-05, atol=1e-08)\n",
    "\n",
    "        a = np.nonzero(m > 0)[0] # find the indices of nonzero elements in the matrix m. [row]\n",
    "        b = np.nonzero(m > 0)[1] # find the indices of nonzero elements in the matrix m. [column]\n",
    "        edge_ind.append(a) # These arrays represent the edge indices of the nonzero elements in the adjacency matrix.\n",
    "        edge_ind.append(b)\n",
    "        return torch.tensor(np.array(edge_ind), dtype= torch.long) # creates a PyTorch tensor from the concatenated arrays in edge_ind, converting it to a long data type tensor\n",
    "\n",
    "\n",
    "    # get structure from a pdb file\n",
    "    # Uses biopython\n",
    "    def _get_structure(self, file):\n",
    "        parser = PDBParser()\n",
    "        structure = parser.get_structure(id, file) # return a Structure object.\n",
    "        return structure\n",
    "\n",
    "    # Function to get sequence from pdb structure\n",
    "    # Uses structure made using biopython\n",
    "    # Those residues for which symbols are U / X are converted into A\n",
    "    def _get_sequence(self, structure):\n",
    "        sequence =\"\"\n",
    "        for model in structure:\n",
    "          for chain in model:\n",
    "            for residue in chain:\n",
    "              if residue.get_resname() in ressymbl.keys():\n",
    "                  sequence = sequence+ ressymbl[residue.get_resname()]\n",
    "        return sequence\n",
    "\n",
    "\n",
    "    # One hot encoding for symbols\n",
    "    def _get_one_hot_symbftrs(self, sequence):\n",
    "        one_hot_symb = np.zeros((len(sequence),len(pro_res_table)))\n",
    "        row= 0\n",
    "        for res in sequence:\n",
    "          col = pro_res_table.index(res)\n",
    "          one_hot_symb[row][col]=1\n",
    "          row +=1\n",
    "        return torch.tensor(one_hot_symb, dtype= torch.float)\n",
    "\n",
    "\n",
    "    # Residue features calculated from pcp_dict\n",
    "    def _get_res_ftrs(self, sequence):\n",
    "        res_ftrs_out = []\n",
    "        for res in sequence:\n",
    "          res_ftrs_out.append(pcp_dict[res])\n",
    "        res_ftrs_out= np.array(res_ftrs_out)\n",
    "        #print(res_ftrs_out.shape)\n",
    "        return torch.tensor(res_ftrs_out, dtype = torch.float)\n",
    "\n",
    "\n",
    "    # total features after concatenating one_hot_symbftrs and res_ftrs\n",
    "    def _get_node_ftrs(self, sequence):\n",
    "        one_hot_symb = one_hot_symbftrs(sequence)\n",
    "        res_ftrs_out = res_ftrs(sequence)\n",
    "        return torch.tensor(np.hstack((one_hot_symb, res_ftrs_out)), dtype = torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HabVZ86yLert"
   },
   "outputs": [],
   "source": [
    "# prot_graphs = ProteinDataset(root=\"R963A_cluster/\")\n",
    "prot_graphs = ProteinDataset(root=\"WT_cluster/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Node feature shape: torch.Size([11530, 1024])\n",
    "The node features for all nodes in the batch (10*1153), with each node having 1024 features. The first dimension (11530) represents the total number of nodes across all graphs in the batch.\n",
    "2. Batch tensor: tensor([0, 0, 0,  ..., 9, 9, 9])\n",
    "The batch tensor is a 1-dimensional tensor where each element indicates the graph index for the corresponding node.\n",
    "3. Edge index shape: torch.Size([2, 145596])\n",
    "The edge indices for all edges in the batch. The first dimension (2) is because edges are represented as pairs of node indices (source and target). The second dimension (145734) is the total number of edges in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features_pro, hidden_channels, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "#         torch.manual_seed(12345)\n",
    "#         self.conv1 = GCNConv(num_features_pro, hidden_channels)\n",
    "        self.conv1 = GCNConv(num_features_pro, num_features_pro)\n",
    "#         self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(num_features_pro, hidden_channels)\n",
    "        self.fc1 = Linear(hidden_channels, 128)\n",
    "        \n",
    "        self.fc2 = Linear(128, 64)\n",
    "        self.out = Linear(64, num_classes)\n",
    "        self.relu = torch.nn.LeakyReLU()\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "#         x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # add some dense layers\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.out(x)\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/souviksinha/Desktop/Palermo_Lab/LabWork/GNN_classifier/PPI_GNN/test/crRNA_cluster/processed/cluster_c3.pt'\n",
    "data = torch.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.x)\n",
    "print(data.batch)\n",
    "print(data.edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load the model state and run the predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(num_features_pro=1024, hidden_channels=256, num_classes=2)\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # for data in loader:  # Iterate in batches over the dataset.\n",
    "    data = data.to(device)\n",
    "    out = model(data.x, data.edge_index, data.batch) \n",
    "    \n",
    "    # Use softmax to get probabilities\n",
    "    prob = torch.softmax(out, dim=1)\n",
    "    pred = prob.argmax(dim=1)  # Use the class with highest probability\n",
    "    \n",
    "    # y_true.extend(data.y.cpu().numpy())\n",
    "    y_pred.extend(pred.cpu().numpy())\n",
    "print(f\"Predicted class of the given input is {y_pred}\")\n",
    "print(f\"Output of the model: {out}\")\n",
    "print(f\"Probability of the output obtained using siftmax: {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.explain import Explainer, GNNExplainer\n",
    "\n",
    "# Define the explainer\n",
    "explainer_pyg = Explainer(\n",
    "    model=model,\n",
    "    algorithm=GNNExplainer(epochs=200),\n",
    "    explanation_type='model',\n",
    "    node_mask_type='attributes',\n",
    "    edge_mask_type='object',\n",
    "    model_config=dict(\n",
    "        mode='binary_classification',\n",
    "        task_level='graph',\n",
    "        return_type='raw',\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Select a sample from the testset\n",
    "# data = testset[0].to(device)\n",
    "\n",
    "# Explain the prediction for this graph\n",
    "explanation = explainer_pyg(data.x, data.edge_index, batch=data.batch)\n",
    "\n",
    "# Print edge mask and node mask\n",
    "print(\"Edge mask:\", explanation.edge_mask)\n",
    "print(\"Node mask:\", explanation.node_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(explanation.node_mask.shape)\n",
    "print(explanation.edge_mask.shape)\n",
    "print(sorted(explanation.edge_mask, reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to explain a single graph\n",
    "def explain_graph(data):\n",
    "    explanation = explainer_pyg(data.x, data.edge_index, batch=data.batch)\n",
    "    return explanation.edge_mask\n",
    "\n",
    "# Collect explanations for multiple graphs\n",
    "edge_masks = []\n",
    "for data in testset:\n",
    "    data = data.to(device)\n",
    "    edge_mask = explain_graph(data)\n",
    "    edge_masks.append(edge_mask.cpu().detach().numpy())\n",
    "\n",
    "# Compute the average edge mask\n",
    "average_edge_mask = np.mean(edge_masks, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(edge_masks[60]))\n",
    "# print(edge_masks)\n",
    "for i, count in enumerate(edge_masks):\n",
    "    print(count.shape)\n",
    "# combined_array = np.stack(edge_masks)\n",
    "\n",
    "# # Sum over the first axis\n",
    "# sum_array = np.sum(combined_array, axis=0)\n",
    "# print(sum_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "def visualize_top_subgraphs(data, output, edge_mask, top_n=10):\n",
    "    # Convert data to networkx graph\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "    \n",
    "    # Set edge weights from edge mask\n",
    "    for i, (u, v) in enumerate(G.edges()):\n",
    "        G[u][v]['weight'] = edge_mask[i].item()\n",
    "    \n",
    "    # Get top-N edges based on weight\n",
    "    top_edges = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)[:top_n]\n",
    "    top_edges = [(u, v) for u, v, w in top_edges]\n",
    "\n",
    "    # Create a subgraph containing the top-N edges\n",
    "    subgraph = G.edge_subgraph(top_edges)\n",
    "    \n",
    "    # Plot the subgraph\n",
    "    pos = nx.spring_layout(subgraph)\n",
    "#     pos = nx.shell_layout(subgraph)\n",
    "    edge_weights = [subgraph[u][v]['weight'] for u, v in subgraph.edges()]\n",
    "    nx.draw(subgraph, pos, with_labels=True, edge_color=edge_weights, edge_cmap=plt.cm.Reds, node_color='lightblue')\n",
    "    \n",
    "   # Add edge labels (weights) on top of the edges\n",
    "#     edge_labels = {(u, v): f'{w:.4f}' for u, v, w in subgraph.edges(data='weight')}\n",
    "#     nx.draw_networkx_edge_labels(subgraph, pos, edge_labels=edge_labels)\n",
    "#     nx.draw_networkx_edge_labels(subgraph, pos, edge_labels={(u, v): f'{w:.2f}' for u, v, w in subgraph.edges(data='weight')})\n",
    "    plt.savefig(output, format=\"PNG\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage (assuming 'data' and 'edge_mask' are defined)\n",
    "visualize_top_subgraphs(data, \"test1_top30.png\", explanation.edge_mask, top_n=30)\n",
    "# visualize_top_subgraphs(data, average_edge_mask, top_n=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize explaination (top n edges) using Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def visualize_top_subgraphs_plotly(data, edge_mask, top_n=10, save_path=None):\n",
    "    # Convert data to networkx graph\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "    \n",
    "    # Set edge weights from edge mask\n",
    "    for i, (u, v) in enumerate(G.edges()):\n",
    "        G[u][v]['weight'] = edge_mask[i].item()\n",
    "    \n",
    "    # Get top-N edges based on weight\n",
    "    top_edges = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)[:top_n]\n",
    "    top_edges = [(u, v) for u, v, w in top_edges]\n",
    "\n",
    "    # Create a subgraph containing the top-N edges\n",
    "    subgraph = G.edge_subgraph(top_edges)\n",
    "\n",
    "    edge_weights = [subgraph[u][v]['weight'] for u, v in subgraph.edges()]\n",
    "    # edge_weights = [float(subgraph[u][v]['weight']) for u, v in subgraph.edges()]\n",
    "\n",
    "    # Get positions for all nodes in subgraph\n",
    "    pos = nx.spring_layout(subgraph)\n",
    "\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    for edge in subgraph.edges(data=True):\n",
    "        # x, y = [], []\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        # x.append(x0)\n",
    "        # x.append(x1)\n",
    "        # x.append(None)\n",
    "\n",
    "        # y.append(y0)\n",
    "        # y.append(y1)\n",
    "        # y.append(None)\n",
    "        \n",
    "        edge_x.append(x0)\n",
    "        edge_x.append(x1)\n",
    "        edge_x.append(None)\n",
    "        edge_y.append(y0)\n",
    "        edge_y.append(y1)\n",
    "        edge_y.append(None)\n",
    "    # print(edge_x)\n",
    "    \n",
    "    # Create Plotly edge traces\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        line=dict(width=5, color='#888'),\n",
    "        hoverinfo='none',\n",
    "        mode='lines'\n",
    "    )\n",
    "\n",
    "    node_x = []\n",
    "    node_y = []\n",
    "    node_text = []\n",
    "    \n",
    "    for node in subgraph.nodes():\n",
    "            x, y = pos[node]\n",
    "            node_x.append(x)\n",
    "            node_y.append(y)\n",
    "            node_text.append(str(node))\n",
    "        \n",
    "    # Create Plotly node traces\n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x,\n",
    "        y=node_y,\n",
    "        text=node_text,\n",
    "        mode='markers+text',\n",
    "        # textposition='top center',\n",
    "        textposition='middle center',\n",
    "        hoverinfo='text',\n",
    "        marker=dict(\n",
    "            showscale=True,\n",
    "            colorscale='YlGnBu',\n",
    "            size=30,\n",
    "            colorbar=dict(\n",
    "                thickness=15,\n",
    "                title='Node Connections',\n",
    "                xanchor='left',\n",
    "                titleside='right'\n",
    "            ),\n",
    "            line_width=2)\n",
    "    )\n",
    "\n",
    "    node_adjacencies = []\n",
    "    node_text = []\n",
    "    for node, adjacencies in enumerate(subgraph.adjacency()):\n",
    "        node_adjacencies.append(len(adjacencies[1]))\n",
    "        node_text.append('# of connections: '+str(len(adjacencies[1])))\n",
    "    \n",
    "    node_trace.marker.color = node_adjacencies\n",
    "    # node_trace.text = node_text\n",
    "\n",
    "\n",
    "\n",
    "    # fig = go.Figure(data=[node_trace]+edge_trace[0:],\n",
    "    fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                    layout=go.Layout(\n",
    "                        # title='<br>Network graph of top subgraph edges',\n",
    "                        # paper_bgcolor='white',\n",
    "                        plot_bgcolor='white',\n",
    "                        titlefont_size=20,\n",
    "                        showlegend=False,\n",
    "                        hovermode='closest',\n",
    "                        margin=dict(b=20, l=5, r=5, t=40),\n",
    "                        annotations=[dict(\n",
    "                            text=f\"Top {top_n} subgraphs\",\n",
    "                            showarrow=False,\n",
    "                            xref=\"paper\", yref=\"paper\",\n",
    "                            x=0.005, y=-0.002)],\n",
    "                        xaxis=dict(showgrid=False, zeroline=False),\n",
    "                        yaxis=dict(showgrid=False, zeroline=False),\n",
    "                        width=1000,\n",
    "                        height=800)\n",
    "                    )\n",
    "\n",
    "    # Save the plot if save_path is provided\n",
    "    if save_path:\n",
    "        fig.write_image(save_path)\n",
    "    fig.show()\n",
    "\n",
    "# Example usage (assuming 'data' and 'edge_mask' are defined)\n",
    "visualize_top_subgraphs_plotly(data, explanation.edge_mask, top_n=40, save_path='top40_crRNA_cluster5.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 660 6 667 5 32 7 676 664 663 8 2 21 20 22 19 11 3 9 25 147 16 363 361 12 17 18 10 146 151\n",
    "# 30 48 63 31 96 35 33 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Getting isomorphic graphs/sub graphs from the individual cluster graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import isomorphism\n",
    "from networkx.algorithms.isomorphism import ISMAGS\n",
    "\n",
    "def find_common_subgraphs(graphs):\n",
    "    \"\"\"\n",
    "    Find common subgraphs between multiple NetworkX graphs.\n",
    "\n",
    "    Parameters:\n",
    "    graphs (list): List of NetworkX graphs.\n",
    "\n",
    "    Returns:\n",
    "    list: List of common subgraphs as NetworkX graphs.\n",
    "    \"\"\"\n",
    "    common_subgraphs = []\n",
    "\n",
    "    if not graphs:\n",
    "        return common_subgraphs\n",
    "\n",
    "    base_graph = graphs[0]\n",
    "\n",
    "    for graph in graphs[1:]:\n",
    "        gm = ISMAGS(base_graph, graph)\n",
    "        for subgraph_nodes in gm.find_isomorphisms():\n",
    "            subgraph = base_graph.subgraph(subgraph_nodes.keys())\n",
    "            common_subgraphs.append(subgraph)\n",
    "        # gm = isomorphism.GraphMatcher(base_graph, graph)\n",
    "        # for subgraph_nodes in gm.subgraph_isomorphisms_iter():\n",
    "        #     subgraph = base_graph.subgraph(subgraph_nodes.keys())\n",
    "        #     common_subgraphs.append(subgraph)\n",
    "\n",
    "    return common_subgraphs\n",
    "\n",
    "# Example usage\n",
    "# G1 = nx.fast_gnp_random_graph(10, 0.5, seed=1)\n",
    "# G2 = nx.fast_gnp_random_graph(9, 0.5, seed=1)\n",
    "# G3 = nx.fast_gnp_random_graph(8, 0.5, seed=1)\n",
    "\n",
    "# common_subgraphs = find_common_subgraphs([G1, G2, G3])\n",
    "\n",
    "common_subgraphs = find_common_subgraphs([c4_graph, c3_graph])\n",
    "\n",
    "for i, subgraph in enumerate(common_subgraphs):\n",
    "    print(f\"Common Subgraph {i+1}:\")\n",
    "    print(subgraph.nodes())\n",
    "    print(subgraph.edges())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c4_graph, c3_graph)\n",
    "print(common_subgraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_subgraphs(subgraphs):\n",
    "    for i, subgraph in enumerate(subgraphs):\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        pos = nx.spring_layout(subgraph)\n",
    "        nx.draw(subgraph, pos, with_labels=True, node_color='lightblue', edge_color='gray')\n",
    "        plt.title(f\"Common Subgraph {i+1}\")\n",
    "        plt.show()\n",
    "\n",
    "# Visualize the common subgraphs\n",
    "visualize_subgraphs(common_subgraphs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the prediction on all the cluster centers, compose all cluster graphs to one & also look for intrsections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.graph_objs as go\n",
    "from torch_geometric.explain import Explainer, GNNExplainer\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "## Return a subgraph containing top N edges\n",
    "def top_subgraphs(data, edge_mask, top_n=30):\n",
    "    # Convert data to networkx graph\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "    \n",
    "    # Set edge weights from edge mask\n",
    "    for i, (u, v) in enumerate(G.edges()):\n",
    "        G[u][v]['weight'] = edge_mask[i].item()\n",
    "    \n",
    "    # Get top-N edges based on weight\n",
    "    top_edges = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)[:top_n]\n",
    "    top_edges = [(u, v) for u, v, w in top_edges]\n",
    "    \n",
    "    # Create a subgraph containing the top-N edges\n",
    "    subgraph = G.edge_subgraph(top_edges)\n",
    "    return subgraph\n",
    "\n",
    "def get_graphs(pt_files, model_state, top_n=30):\n",
    "    # model = GCN(num_features_pro=1024, hidden_channels=256, num_classes=2)\n",
    "    model = GCN(num_features_pro=1024, hidden_channels=256, num_classes=3)\n",
    "    model.load_state_dict(torch.load(model_state))\n",
    "\n",
    "    graphs = []\n",
    "\n",
    "    # Define the explainer\n",
    "    explainer_pyg = Explainer(\n",
    "        model=model,\n",
    "        algorithm=GNNExplainer(epochs=200),\n",
    "        explanation_type='model',\n",
    "        node_mask_type='attributes',\n",
    "        edge_mask_type='object',\n",
    "        model_config=dict(\n",
    "            mode='binary_classification',\n",
    "            task_level='graph',\n",
    "            return_type='raw',\n",
    "        ),\n",
    "    )\n",
    "    for file in pt_files:\n",
    "        data = torch.load(file)\n",
    "\n",
    "        ## Evaluate output\n",
    "        y_pred = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # for data in loader:  # Iterate in batches over the dataset.\n",
    "            # data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.batch) \n",
    "            \n",
    "            # Use softmax to get probabilities\n",
    "            prob = torch.softmax(out, dim=1)\n",
    "            pred = prob.argmax(dim=1)  # Use the class with highest probability\n",
    "            \n",
    "            # y_true.extend(data.y.cpu().numpy())\n",
    "            y_pred.extend(pred.cpu().numpy())\n",
    "        print(f\"Predicted class of the given input is {y_pred}\")\n",
    "        print(f\"Output of the model: {out}\")\n",
    "        print(f\"Probability of the output obtained using softmax: {prob}\")\n",
    "\n",
    "        # Explain the prediction for this graph\n",
    "        explanation = explainer_pyg(data.x, data.edge_index, batch=data.batch)\n",
    "\n",
    "        graph = top_subgraphs(data, explanation.edge_mask, top_n=top_n)\n",
    "        graphs.append(graph)\n",
    "        name = file.split('/')[-1].split('.')[0]\n",
    "        visualize_graph_plotly(graph, save_path= os.path.join(workdir, f'top{top_n}_{name}.png'))\n",
    "\n",
    "    return graphs\n",
    "\n",
    "def visualize_graph_plotly(graph, node_dist = None, save_path=None):\n",
    "    pos = nx.spring_layout(graph, k = node_dist)\n",
    "    \n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    for edge in graph.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x.append([x0, x1, None])\n",
    "        edge_y.append([y0, y1, None])\n",
    "    \n",
    "    edge_trace = go.Scatter(\n",
    "        x=[x for edge in edge_x for x in edge],\n",
    "        y=[y for edge in edge_y for y in edge],\n",
    "        line=dict(width=3, color='gray'),\n",
    "        hoverinfo='none',\n",
    "        mode='lines')\n",
    "    \n",
    "    node_x = [pos[node][0] for node in graph.nodes()]\n",
    "    node_y = [pos[node][1] for node in graph.nodes()]\n",
    "    node_degree = [graph.degree[node] for node in graph.nodes()]\n",
    "    \n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x, y=node_y,\n",
    "        mode='markers+text',\n",
    "        text=[str(node) for node in graph.nodes()],\n",
    "        # textposition='top center',\n",
    "        textposition='middle center',\n",
    "        hoverinfo='text',\n",
    "        marker=dict(\n",
    "            showscale=True,\n",
    "            # colorscale='YlGnBu',\n",
    "            colorscale='tealgrn',\n",
    "            size=30,\n",
    "            color = node_degree,\n",
    "            # cmin = 0,\n",
    "            # cmax = 6,\n",
    "            colorbar=dict(\n",
    "                thickness=15,\n",
    "                title='Node Connections',\n",
    "                xanchor='left',\n",
    "                titleside='right',\n",
    "            ),\n",
    "            line_width=2))\n",
    "    \n",
    "    \n",
    "    fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                    layout=go.Layout(\n",
    "                        title='Intersection Graph',\n",
    "                        plot_bgcolor='white',\n",
    "                        titlefont_size=16,\n",
    "                        showlegend=False,\n",
    "                        hovermode='closest',\n",
    "                        margin=dict(b=20, l=5, r=5, t=40),\n",
    "                        annotations=[dict(\n",
    "                            text=\" \",\n",
    "                            showarrow=False,\n",
    "                            xref=\"paper\", yref=\"paper\",\n",
    "                            x=0.005, y=-0.002)],\n",
    "                        xaxis=dict(showgrid=False, zeroline=False),\n",
    "                        yaxis=dict(showgrid=False, zeroline=False),\n",
    "                    width=1000,\n",
    "                        height=1000)\n",
    "                   )\n",
    "    fig.update_layout(\n",
    "        yaxis = dict(tickfont = dict(size=20)),\n",
    "        xaxis = dict(tickfont = dict(size=20)))\n",
    "    \n",
    "    # Save the plot if save_path is provided\n",
    "    if save_path:\n",
    "        fig.write_image(save_path)\n",
    "    fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workdir = '/Users/souviksinha/Desktop/Palermo_Lab/LabWork/GNN_classifier/PPI_GNN/WT_R963A_test/R963A_cluster/processed/'\n",
    "workdir = '/Users/souviksinha/Desktop/Palermo_Lab/LabWork/GNN_classifier/PPI_GNN/Chinmai_proj/win0_vs_win16_win56/frame_1/processed/'\n",
    "pt_files = glob.glob(os.path.join(workdir,'frame*.pt'))\n",
    "model_state = 'best_model_GCNN.pt'\n",
    "\n",
    "graphs_state0 = get_graphs(pt_files, model_state, top_n=60 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workdir = '/Users/souviksinha/Desktop/Palermo_Lab/LabWork/GNN_classifier/PPI_GNN/WT_R963A_test/R963A_cluster/processed/'\n",
    "workdir = '/Users/souviksinha/Desktop/Palermo_Lab/LabWork/GNN_classifier/PPI_GNN/Chinmai_proj/win0_vs_win16_win56/frame_16/processed/'\n",
    "pt_files = glob.glob(os.path.join(workdir,'frame*.pt'))\n",
    "model_state = 'best_model_GCNN.pt'\n",
    "\n",
    "graphs_state1 = get_graphs(pt_files, model_state, top_n=60 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workdir = '/Users/souviksinha/Desktop/Palermo_Lab/LabWork/GNN_classifier/PPI_GNN/WT_R963A_test/R963A_cluster/processed/'\n",
    "workdir = '/Users/souviksinha/Desktop/Palermo_Lab/LabWork/GNN_classifier/PPI_GNN/Chinmai_proj/win0_vs_win16_win56/frame_56/processed/'\n",
    "pt_files = glob.glob(os.path.join(workdir,'frame*.pt'))\n",
    "model_state = 'best_model_GCNN.pt'\n",
    "\n",
    "graphs_state2 = get_graphs(pt_files, model_state, top_n=60 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_top_subgraphs_plotly(data, explanation.edge_mask, top_n=40, save_path='top40_crRNA_cluster5.png')\n",
    "# for file in pt_files:\n",
    "#     print(file)\n",
    "#     data = torch.load(file)\n",
    "#     print(file.split('/')[-1].split('.')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "from collections import Counter\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "workdir = '/Users/souviksinha/Desktop/Palermo_Lab/LabWork/GNN_classifier/PPI_GNN/Chinmai_proj/win0_vs_win16_win56/'\n",
    "\n",
    "all_edges = []\n",
    "for G in graphs_state0:\n",
    "    all_edges.extend(G.edges)\n",
    "\n",
    "# Step 2: Count occurrences of each edge using a Counter\n",
    "edge_counts = Counter(all_edges)\n",
    "# print(edge_counts)\n",
    "\n",
    "# Step 3: Create a new graph for the intersection based on the criterion (minimum 3 graphs)\n",
    "G_intersection = nx.Graph()\n",
    "\n",
    "## Calculate frequency edges appeared in top-n edge list\n",
    "x_ax, y_ax, z = [], [], []\n",
    "for edge, count in edge_counts.items():\n",
    "    # print(edge[0], edge[1], count)\n",
    "    x_ax.append(edge[0])\n",
    "    y_ax.append(edge[1])\n",
    "    z.append(count/100)\n",
    "\n",
    "## Plot\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "new_z = [i*100 for i in z]\n",
    "# Create the scatter plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# plt.figure(figsize=(8,6))\n",
    "# sc = ax.scatter(x_ax, y_ax, c=z, s=new_z, linewidth=0.2, edgecolors='black', cmap='viridis_r', alpha=0.8)\n",
    "sc = ax.scatter(x_ax, y_ax, c=z, s=new_z, linewidth=0.2, edgecolors='black', cmap='viridis_r', alpha=0.7)\n",
    "\n",
    "# Add a color bar\n",
    "# plt.colorbar(sc, label='Frequency')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlim(0, 552)\n",
    "plt.ylim(0, 552)\n",
    "\n",
    "plt.xlabel('Dimer', fontsize = 24, fontweight = 'bold')\n",
    "plt.ylabel('Dimer', fontsize = 24, fontweight = 'bold')\n",
    "\n",
    "# Add horizontal and vertical lines\n",
    "monomer_A = [55,65,95,138,146,197,275]  # Vertical line at x=5\n",
    "monomer_B = [i+276 for i in monomer_A]  # Horizontal line at y=7\n",
    "\n",
    "text_loc_A = [25, 60, 80, 120, 142, 170, 230]\n",
    "text_loc_B = [i+276 for i in text_loc_A]\n",
    "\n",
    "domain = ['N-ter', 'Walker-A', 'A-ISM linker', 'ISM', 'Walker-B', 'B-Cter linker', 'C-ter']\n",
    "for j,i in enumerate(monomer_A):\n",
    "    plt.axvline(x=i, color='r', linestyle='--', linewidth=0.5)\n",
    "    plt.text(text_loc_A[j] , 555, f'{domain[j]}', color='r', rotation=45, fontsize=9)\n",
    "\n",
    "    plt.axhline(y=i, color='r', linestyle='-.', linewidth=0.5)\n",
    "    plt.text(555, text_loc_A[j], f'{domain[j]}', color='r',rotation=45, fontsize=9)\n",
    "\n",
    "for j,i in enumerate(monomer_B):\n",
    "    plt.axvline(x=i, color='b', linestyle='--', linewidth=0.5)\n",
    "    plt.text(text_loc_B[j] , 555, f'{domain[j]}', color='b', rotation=45, fontsize=9)\n",
    "    \n",
    "    plt.axhline(y=i, color='b', linestyle='-.', linewidth=0.5)\n",
    "    plt.text(555, text_loc_B[j], f'{domain[j]}', color='b', rotation=45, fontsize=9)\n",
    "\n",
    "plt.axvline(x=275, color='black', linestyle='--', linewidth=1)\n",
    "plt.text(275-10 , 555, f'MonomerA->B', color='black', rotation=45, fontsize=12)\n",
    "\n",
    "plt.axhline(y=275, color='black', linestyle='-.', linewidth=1)\n",
    "plt.text(555, 275-2, f'MonomerA->B', color='black',rotation=45, fontsize=12)\n",
    "\n",
    "# Create a divider to adjust the position of the colorbar\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=1.2)  # You can adjust 'size' and 'pad' to move the colorbar\n",
    "\n",
    "# Add the colorbar to the custom axes\n",
    "cbar = fig.colorbar(sc, cax=cax)\n",
    "cbar.set_label('Frequency')\n",
    "\n",
    "# plt.show()\n",
    "# plt.xticks([i for i in range(0, 10)], [5*i for i in range(1, 11)], fontsize = 22)\n",
    "plt.xticks(fontsize = 22)\n",
    "plt.yticks(fontsize = 22)\n",
    "plt.legend(frameon = False, fontsize = 20)\n",
    "plt.savefig(workdir + 'state_0_top60.png', dpi = 600, bbox_inches = 'tight')\n",
    "\n",
    "## Filter out common top-ranked edges\n",
    "# for edge, count in edge_counts.items():\n",
    "    # if count >= 40:  # Minimum intersection criterion: 3 graphs\n",
    "    #     G_intersection.add_edge(*edge)\n",
    "\n",
    "# # Print the edges of the intersection graph\n",
    "# print(\"Edges in the intersection graph (appearing in at least 50 graphs):\")\n",
    "# print(list(G_intersection.edges))\n",
    "\n",
    "# # Visualize the graph (optional)\n",
    "# import matplotlib.pyplot as plt\n",
    "# # nx.draw(G_intersection, with_labels=True, node_color='lightblue', edge_color='red')\n",
    "# # plt.show()\n",
    "# visualize_graph_plotly(G_intersection, save_path='state0_top30edges_intersection_cutoff40.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For state 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workdir = '/Users/souviksinha/Desktop/Palermo_Lab/LabWork/GNN_classifier/PPI_GNN/WT_R963A_test/R963A_cluster/processed/'\n",
    "workdir = '/Users/souviksinha/Desktop/Palermo_Lab/LabWork/GNN_classifier/PPI_GNN/Chinmai_proj/frame_56/processed/'\n",
    "pt_files = glob.glob(os.path.join(workdir,'frame_*.pt'))\n",
    "model_state = 'best_model.pt'\n",
    "\n",
    "graphs = get_graphs(pt_files, model_state, top_n=30 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "from collections import Counter\n",
    "\n",
    "all_edges = []\n",
    "for G in graphs:\n",
    "    all_edges.extend(G.edges)\n",
    "\n",
    "# Step 2: Count occurrences of each edge using a Counter\n",
    "edge_counts = Counter(all_edges)\n",
    "# print(edge_counts)\n",
    "\n",
    "# Step 3: Create a new graph for the intersection based on the criterion (minimum 3 graphs)\n",
    "G_intersection = nx.Graph()\n",
    "\n",
    "for edge, count in edge_counts.items():\n",
    "    if count >= 5:  # Minimum intersection criterion: 3 graphs\n",
    "        G_intersection.add_edge(*edge)\n",
    "\n",
    "# Print the edges of the intersection graph\n",
    "print(\"Edges in the intersection graph (appearing in at least 50 graphs):\")\n",
    "print(list(G_intersection.edges))\n",
    "\n",
    "# Visualize the graph (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "# nx.draw(G_intersection, with_labels=True, node_color='lightblue', edge_color='red')\n",
    "# plt.show()\n",
    "visualize_graph_plotly(G_intersection, save_path='/Users/souviksinha/Desktop/Palermo_Lab/LabWork/GNN_classifier/PPI_GNN/Chinmai_proj/frame_56/processed/state1_top30edges_intersection_cutoff5.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C = nx.compose_all([c4_graph, c3_graph, c5_graph])\n",
    "# print(list(C.nodes()))\n",
    "\n",
    "# print(list(C.edges()))\n",
    "# visualize_graph_plotly(C, node_dist=0.14, save_path='test_compose.png' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "C = nx.compose_all(graphs)\n",
    "print(list(C.nodes()))\n",
    "\n",
    "print(list(C.edges()))\n",
    "# visualize_graph_plotly(C, node_dist=0.1, save_path='cluster0-10_top30edges_compose.png' )\n",
    "visualize_graph_plotly(C, node_dist=0.6, save_path='frame1*_top30edges_compose.png' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0, 1, 2, 3, 4, 5, 8, 136, 10, 139, 9, 140, 143, 19, 147, 29, 32, 33, 36, 37, 1028, 133, 398, 400, 404, 406, 663, 408, 407, 665, 410, 1052, 1057, 417, 419, 421, 423, 180, 181, 183, 184, 75, 78, 79, 80, 81, 82, 83, 84, 85, 86, 340, 88, 87, 92, 93, 113, 115, 118, 119, 120, 123, 127, 64, 35, 39, 40, 42, 43, 309, 137, 396, 403, 550, 551, 426, 182, 185, 186, 187, 188, 189, 193, 72, 73, 74, 76, 77, 464, 336, 103, 104, 105, 114, 117, 68, 47, 71, 38]\n",
    "sele = \"resid \" + ' '.join(map(str, [i+1 for i in a])) + ' and not name C N O and noh'\n",
    "print(sele)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
